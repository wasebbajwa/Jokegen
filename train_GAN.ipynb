{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "generator_model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "generator_model = generator_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = tokenizer.encode('man')\n",
    "vector = generator_model.transformer.wte.weight[text_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' man'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(582)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_jokes(num_jokes=1,return_embeddings=True):\n",
    "    model = generator_model\n",
    "    joke_num=0\n",
    "    jokes = []\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        for joke_idx in range(num_jokes):\n",
    "        \n",
    "            joke_finished = False\n",
    "\n",
    "            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(100):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "                if i < 3:\n",
    "                    n = 20\n",
    "                else:\n",
    "                    n = 3\n",
    "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                    joke_finished = True\n",
    "                    break\n",
    "\n",
    "            joke_finished=True\n",
    "            if joke_finished:\n",
    "                \n",
    "                joke_num = joke_num + 1\n",
    "                output_tensor = cur_ids.squeeze().to(device)\n",
    "                if not return_embeddings:\n",
    "                    output_list = list(output_tensor.numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    jokes.append(output_text)\n",
    "                else:\n",
    "                    joke_embedding = [generator_model.transformer.wte.weight[text_index,:] for text_index in output_tensor]\n",
    "                    jokes.append(torch.stack(joke_embedding))\n",
    "    if return_embeddings:\n",
    "        jokes = torch.stack(jokes)\n",
    "    return jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 103, 1024])\n"
     ]
    }
   ],
   "source": [
    "jokes = generate_jokes(2,return_embeddings=True)\n",
    "print(jokes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes_dataset_path = 'data/'):\n",
    "        super().__init__()\n",
    "\n",
    "        short_jokes_path = os.path.join(jokes_dataset_path, 'dadjokesfinal.csv')\n",
    "\n",
    "        self.joke_list = []\n",
    "        self.end_of_text_token = \"\\r\"\n",
    "        \n",
    "        with open(short_jokes_path) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            \n",
    "            x = 0\n",
    "            for row in csv_reader:\n",
    "                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n",
    "                self.joke_list.append(joke_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.joke_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.joke_list[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=103\n",
    "embedding_dim = 1024\n",
    "hidden_dim=128\n",
    "batch_size=2\n",
    "\n",
    "\n",
    "dataset = JokesDataset()\n",
    "joke_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#discriminator_model = torch.nn.Sequential(\n",
    "#    torch.nn.GRU(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True),\n",
    "#    torch.nn.Linear(hidden_dim * 2 * 2, 1),\n",
    "#    torch.nn.Sigmoid()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim,num_layers,batch_first):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.gru = torch.nn.GRU(input_size=embedding_dim,hidden_size=hidden_dim,num_layers=1,batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.linear(hidden)\n",
    "        output = self.activation(x)\n",
    "        return output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(n_iters,verbose=False):\n",
    "    data_iterator = iter(joke_loader)\n",
    "    generator = generator_model\n",
    "    discriminator = Discriminator(embedding_dim=embedding_dim, hidden_dim=hidden_dim,num_layers=1,batch_first=True)\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "    loss = torch.nn.BCELoss()\n",
    "    for i in range(n_iters):\n",
    "        if verbose:\n",
    "            print(f\"#### ITERATION {i} ####\")\n",
    "        \n",
    "        #train generator\n",
    "        if verbose:\n",
    "            print(\"training generator\")\n",
    "        generator_optimizer.zero_grad()\n",
    "        gen_examples = generate_jokes(batch_size,return_embeddings=True)\n",
    "        discriminator_preds = discriminator(gen_examples)\n",
    "        print(discriminator_preds)\n",
    "        print(discriminator_preds.shape)\n",
    "        #generator wants discriminator to predict these as true\n",
    "        labels = torch.ones(len(gen_examples))\n",
    "        generator_loss = loss(discriminator_preds, torch.ones(len(labels)))\n",
    "        if verbose:\n",
    "            print(f\"generator loss: {generator_loss}\")\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"training discriminator on true examples\")\n",
    "        #compute discriminator loss on true examples\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        #create list of plain text jokes\n",
    "        true_examples_text = data_iterator.next()\n",
    "        #create list of array indices corresponding to embedding dict\n",
    "        true_examples_vector = [tokenizer.encode(joke) for joke in true_examples_text] \n",
    "        #create list of embeddings tensors using embedding dict\n",
    "        true_examples_list = [generator_model.transformer.wte.weight[text_index,:] for text_index in true_examples_vector]\n",
    "        #create tensor of embedded jokes padded to same length\n",
    "        true_examples = torch.nn.utils.rnn.pad_sequence(true_examples_list,batch_first=True)\n",
    "        \n",
    "        discriminator_preds_on_true = discriminator(true_examples)\n",
    "        #discriminator wants to predict these as true\n",
    "        true_labels = torch.ones(len(true_examples))\n",
    "        discriminator_loss_on_true = loss(discriminator_preds_on_true, true_labels)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"discriminator loss on true examples: {discriminator_loss_on_true}\")\n",
    "            print(\"training discriminator on generated examples\")\n",
    "        #compute discriminator  loss on generated examples\n",
    "        discriminator_preds_on_gen = discriminator(gen_examples)\n",
    "        #discriminator wants to predict these as false\n",
    "        gen_labels = torch.zeros(len(gen_examples))\n",
    "        discriminator_loss_on_gen = loss(discriminator_preds_on_gen, gen_labels)\n",
    "        if verbose:\n",
    "            print(f\"discriminator loss on generated examples: {discriminator_loss_on_gen}\")\n",
    "        \n",
    "        discriminator_loss = (discriminator_loss_on_true + discriminator_loss_on_gen) / 2\n",
    "        if verbose:\n",
    "            print(f\"total discriminator loss: {discriminator_loss}\")\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### ITERATION 0 ####\n",
      "training generator\n"
     ]
    }
   ],
   "source": [
    "train_gan(1,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
