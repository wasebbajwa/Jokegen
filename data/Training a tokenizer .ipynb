{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6954c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import pandas as pd\n",
    "import argparse,sys,os\n",
    "from time import sleep\n",
    "import regex as re\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d360951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('C:/Users/wbajw/MSDS 458/MSDS 460 Project/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bebd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_excel('train.xlsx')\n",
    "dataframe.Transcript.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "dataframe.Transcript.replace({r'\\W +' :' '},regex=True,inplace=True)\n",
    "dataframe.Title.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "dataframe.Title.replace({r'\\W +' :' '},regex=True,inplace=True)\n",
    "dataframe=dataframe[['Title','Transcript']]\n",
    "dataframe.to_csv('traincleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e85941c-0ed9-4bed-a018-74c0b402cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_excel('test.xlsx')\n",
    "dataframe.Transcript.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "dataframe.Transcript.replace({r'\\W +' :' '},regex=True,inplace=True)\n",
    "dataframe.Title.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "dataframe.Title.replace({r'\\W +' :' '},regex=True,inplace=True)\n",
    "dataframe=dataframe[['Title','Transcript']]\n",
    "dataframe.to_csv('testcleaned.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f13c614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    transcript of tom segura’s new netflix special...\n",
       "1    ladies and gentlemen live from the world-famou...\n",
       "2    ladies and gentlemen live from the world-famou...\n",
       "3    filmed in 2020 at the tuacahn outdoor amphithe...\n",
       "4    filmed in 2020 at the tuacahn outdoor amphithe...\n",
       "Name: Transcript, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PLaying around with custom tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transcript=dataframe['Transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1148a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = 'Values.txt'\n",
    "WorkingFolder='C:/Users/wbajw/msds 498'\n",
    "with open(os.path.join(WorkingFolder, base_filename),'w',encoding=\"utf-8\") as outfile:\n",
    "    Transcript.to_string(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55ca7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "paths='Values.txt'\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a8f8e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jokes-vocab.json', 'Jokes-merges.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save files to disk\n",
    "tokenizer.save_model('','Jokes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82d04f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"Jokes-vocab.json\",\n",
    "    'Jokes-merges.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0139edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51ae3108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"There was a black cat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cf196cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'T',\n",
       " 'here',\n",
       " 'Ġ',\n",
       " 'was',\n",
       " 'Ġ',\n",
       " 'a',\n",
       " 'Ġ',\n",
       " 'b',\n",
       " 'la',\n",
       " 'ck',\n",
       " 'Ġ',\n",
       " 'c',\n",
       " 'at',\n",
       " 'Ġ',\n",
       " 'over',\n",
       " 'Ġ',\n",
       " 'there',\n",
       " 'Ġ',\n",
       " 's',\n",
       " 'a',\n",
       " 'id',\n",
       " 'Ġ',\n",
       " 'ch',\n",
       " 'ap',\n",
       " 'e',\n",
       " 'le',\n",
       " '.',\n",
       " '</s>',\n",
       " 'he',\n",
       " 'Ġ',\n",
       " 'was',\n",
       " 'Ġ',\n",
       " 'co',\n",
       " 'ol',\n",
       " '</s>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"There was a black cat over there said chapele.\",'he was cool').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9462b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
